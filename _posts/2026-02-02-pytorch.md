---
title: "Pytorch note"
date: 2025-02-02
categories: [Python, PyTorch]
tags: [python, pyTorch]     # TAG names should always be lowercase
---

## Before begin
### In PyTorch, they call all 1D, 2D, 3D a tensor.

---

## 1. Tensor Creation

### From Python List
You can create a tensor directly from a standard Python list.

```python
import torch
import numpy as np

list_data = [[10, 20], [30, 40]]

# Create tensor from list
tensor1 = torch.Tensor(list_data)

print(tensor1)
# tensor([[10., 20.],
#         [30., 40.]])

# Check attributes
print(f"shape: {tensor1.shape}")   # Shape of the tensor
print(f"dtype: {tensor1.dtype}")   # Data type (default is float32)
print(f"device: {tensor1.device}") # Device location (cpu/cuda)

```

{: .prompt-tip }

> **Using GPU**: If `torch.cuda.is_available()` returns `True`, you can move the tensor to the GPU memory using `.to("cuda")`.

### From NumPy

When converting a NumPy array to a tensor, use `torch.from_numpy()`.

```python
numpy_data = np.array(list_data)
tensor2 = torch.from_numpy(numpy_data)

print(f"dtype: {tensor2.dtype}") 
# Result: torch.int64 (Inherits the integer type from NumPy)

```

{: .prompt-warning }

> **Watch out for Data Types**: Tensors created from NumPy integer arrays default to `int64`. Since Deep Learning models typically require floating-point numbers, it is essential to cast the type using `.float()`.

```python
# Cast to float during creation
tensor2 = torch.from_numpy(numpy_data).float()

```

### Random Data Generation

These functions are frequently used for initializing weights in neural networks.

| Function | Description | Distribution |
| --- | --- | --- |
| `torch.rand(m, n)` | Generates values between 0 and 1 | Uniform Distribution |
| `torch.randn(m, n)` | Generates values with Mean=0, Var=1 | Normal (Gaussian) Distribution |

---

## 2. Tensor Operations

### Indexing & Slicing

Used to access specific elements or extract sub-tensors.

**Base Data (`tensor6`, `tensor7`)**

| `tensor6` | 1 | 2 | 3 |
| --- | --- | --- | --- |
| **Row 1** | 4 | 5 | 6 |

| `tensor7` | 7 | 8 | 9 |
| --- | --- | --- | --- |
| **Row 1** | 10 | 11 | 12 |

**Slicing Examples**

```python
# 1. All data in the first row
print(tensor6[0]) 
# [1., 2., 3.]

# 2. All rows, from the second column to the end
print(tensor6[:, 1:]) 
# [[2., 3.],
#  [5., 6.]]

# 3. Intersection: Rows 0~1 and Columns 0~1
print(tensor7[0:2, 0:-1])
# [[ 7.,  8.],
#  [10., 11.]]

```

### Multiplication: Element-wise vs Matrix Multiplication

It is important to distinguish between simple element-wise multiplication and matrix multiplication (dot product).

1. **`mul` (Element-wise Product)**: Multiplies elements at the same position. (Same as `*` operator).
2. **`matmul` (Matrix Multiplication)**: Performs matrix multiplication. (Same as `@` operator).

```python
# Element-wise product
tensor8 = tensor6.mul(tensor7) 
# Result shape: (2, 3) remains the same

# Matrix Multiplication (Error Case)
# tensor6(2x3) @ tensor7(2x3) -> Shape Mismatch Error!
# The columns of the first matrix (3) must match the rows of the second (2).

```

{: .prompt-info }

> **Reshape (`view`)**: To perform matrix multiplication, we need to change the shape of `tensor7` to `(3, 2)`.

```python
# Perform matrix multiplication after reshaping
# (2, 3) @ (3, 2) -> (2, 2) result
tensor9 = tensor6.matmul(tensor7.view(3, 2))

```

---

## 3. Tensor Concatenation

You can join tensors using `torch.cat`. The `dim` parameter determines the direction of concatenation.

### `dim=0` (Vertical Concatenation)

Stacks tensors vertically (along the rows). The number of columns remains the same.

```python
torch.cat([tensor6, tensor7], dim=0)

```

| Result | Col 0 | Col 1 | Col 2 |
| --- | --- | --- | --- |
| **t6** | 1 | 2 | 3 |
| **t6** | 4 | 5 | 6 |
| **t7** | 7 | 8 | 9 |
| **t7** | 10 | 11 | 12 |

### `dim=1` (Horizontal Concatenation)

Stacks tensors horizontally (along the columns). The number of rows remains the same.

```python
torch.cat([tensor6, tensor7], dim=1)

```

| Result | t6 | t6 | t6 | t7 | t7 | t7 |
| --- | --- | --- | --- | --- | --- | --- |
| **Row 0** | 1 | 2 | 3 | 7 | 8 | 9 |
| **Row 1** | 4 | 5 | 6 | 10 | 11 | 12 |


---


## 1. Overview of Neural Network Structure

A Neural Network for deep learning is composed of various **layers** that perform data operations to calculate predictions.

* **PyTorch API Hierarchy**:
    * **High-Level**: `torch.nn` (Layers, Loss functions), `torch.optim` (Optimizers).
    * **Low-Level**: PyTorch Engine, Hardware (CPU/GPU/MPS).

The overall training workflow follows a cycle: **Data Definition $\rightarrow$ Model Construction $\rightarrow$ Feed Forward $\rightarrow$ Loss Calculation $\rightarrow$ Optimization**.

---

## 2. Step-by-Step Implementation

### Step 1: Data Definition
Since the basic data type in PyTorch is a Tensor, all data must first be converted into Tensors. For more efficient handling (mini-batching, shuffling), we use `TensorDataset` and `DataLoader`.

```python
import torch
from torch.utils.data import TensorDataset, DataLoader

# 1. Create Raw Tensors (Reshaping to (N, 1) is common for regression)
x_train = torch.Tensor([1, 2, 3, 4, 5, 6]).view(6, 1)
y_train = torch.Tensor([3, 4, 5, 6, 7, 8]).view(6, 1)

# 2. Create Dataset and DataLoader
dataset = TensorDataset(x_train, y_train)

# DataLoader handles batch_size and shuffling automatically
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

```

### Step 2: Model Construction (`nn.Module`)

A model is generally defined as a class that inherits from `nn.Module`.

* `__init__`: Define the layers (e.g., `nn.Linear`, `nn.ReLU`, `nn.Sequential`).
* `forward`: Define how data passes through the layers (Feed Forward).

```python
import torch.nn as nn

class MyNeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        # Define layers
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        # Define data flow
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

# Instantiate the model
model = MyNeuralNetwork()

```

### Step 3: Loss Function & Optimizer

To train the model, we need to calculate the error (Loss) and update the parameters to minimize it.

| Component | Description | Examples |
| --- | --- | --- |
| **Loss Function** | Calculates the difference between prediction and ground truth. | `nn.MSELoss` (Regression), `nn.CrossEntropyLoss` (Classification) |
| **Optimizer** | Updates model parameters (weights/biases). | `torch.optim.SGD`, `ADAM`, `RMSProp` |

```python
loss_function = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)

```

### Step 4: Training Loop

The training loop iterates through the dataset multiple times (epochs). In each iteration, the model makes a prediction, calculates the loss, and backpropagates the error to update weights.

{: .prompt-danger }

> **Crucial Steps in the Loop**:
> 1. `optimizer.zero_grad()`: Reset gradients from the previous step.
> 2. `loss.backward()`: Calculate gradients (Backpropagation).
> 3. `optimizer.step()`: Update parameters using the gradients.
> 
> 

---

## 3. Full Example: Linear Regression

Here is a complete example combining all the steps to learn a simple linear relationship.

```python
import torch
import torch.nn as nn

# 1. Data
x_train = torch.Tensor([1, 2, 3, 4, 5, 6]).view(6, 1)
y_train = torch.Tensor([3, 4, 5, 6, 7, 8]).view(6, 1)

# 2. Model (Simple Linear Layer: Input 1 -> Output 1)
class MyNeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(1, 1) # One input, one output
        )
        
    def forward(self, x):
        logits = self.linear_relu_stack(x)
        return logits

model = MyNeuralNetwork()

# 3. Loss & Optimizer
loss_function = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)

# 4. Training Loop
nums_epoch = 2000

for epoch in range(nums_epoch + 1):
    # Prediction
    prediction = model(x_train)
    
    # Calculate Loss
    loss = loss_function(prediction, y_train)
    
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 100 == 0:
        print(f"epoch = {epoch}, current loss = {loss.item()}")

# 5. Testing
# Predict for a new input (e.g., 3.0 should yield approx 5.0)
x_test = torch.Tensor([-3.1, 3.0, 1.2]).view(3, 1)
pred = model(x_test)
print(pred)

```

```

```