{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75333443",
   "metadata": {},
   "source": [
    "# Vanila Deep Learning\n",
    "##### I'm using this page to bridge the gap left by CS 7643 - Deep Learning. I was trying to learn Deep Learning without using libraries in order to deepen my understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494ac0d",
   "metadata": {},
   "source": [
    "##### This Implementations in this notebook follows a very popular deep learning book with ISBN 9791169213387. While full Contents are indicated right under, I will skip the part that I already know such as Python basic syntex etc.\n",
    "\n",
    "Deep Learning from Scratch\n",
    "CHAPTER 1: Hello Python\n",
    "\n",
    "1.1 What is Python?\n",
    "\n",
    "1.2 Installing Python\n",
    "\n",
    "1.3 Python Interpreter\n",
    "\n",
    "1.4 Python Script Files\n",
    "\n",
    "1.5 NumPy\n",
    "\n",
    "1.6 Matplotlib\n",
    "\n",
    "1.7 Summary\n",
    "\n",
    "CHAPTER 2: Perceptrons\n",
    "\n",
    "2.1 What is a Perceptron?\n",
    "\n",
    "2.2 Simple Logic Circuits\n",
    "\n",
    "2.3 Implementing a Perceptron\n",
    "\n",
    "2.4 Limitations of Perceptrons\n",
    "\n",
    "2.5 Multi-Layer Perceptrons\n",
    "\n",
    "2.6 From NAND to Computers\n",
    "\n",
    "2.7 Summary\n",
    "\n",
    "CHAPTER 3: Neural Networks\n",
    "\n",
    "3.1 From Perceptrons to Neural Networks\n",
    "\n",
    "3.2 Activation Functions\n",
    "\n",
    "3.3 Multidimensional Array Calculations\n",
    "\n",
    "3.4 Implementing a 3-Layer Neural Network\n",
    "\n",
    "3.5 Designing the Output Layer\n",
    "\n",
    "3.6 Handwritten Digit Recognition\n",
    "\n",
    "3.7 Summary\n",
    "\n",
    "CHAPTER 4: Neural Network Training\n",
    "\n",
    "4.1 Learning from Data\n",
    "\n",
    "4.2 Loss Functions\n",
    "\n",
    "4.3 Numerical Differentiation\n",
    "\n",
    "4.4 Gradients\n",
    "\n",
    "4.5 Implementing a Training Algorithm\n",
    "\n",
    "4.6 Summary\n",
    "\n",
    "CHAPTER 5: Backpropagation\n",
    "\n",
    "5.1 Computational Graphs\n",
    "\n",
    "5.2 Chain Rule\n",
    "\n",
    "5.3 Backpropagation\n",
    "\n",
    "5.4 Implementing Simple Layers\n",
    "\n",
    "5.5 Implementing Activation Function Layers\n",
    "\n",
    "5.6 Implementing Affine/Softmax Layers\n",
    "\n",
    "5.7 Implementing Backpropagation\n",
    "\n",
    "5.8 Summary\n",
    "\n",
    "CHAPTER 6: Training Techniques\n",
    "\n",
    "6.1 Parameter Updates\n",
    "\n",
    "6.2 Weight Initialization\n",
    "\n",
    "6.3 Batch Normalization\n",
    "\n",
    "6.4 Regularization (For Proper Learning)\n",
    "\n",
    "6.5 Finding Optimal Hyperparameter Values\n",
    "\n",
    "6.6 Summary\n",
    "\n",
    "CHAPTER 7: Convolutional Neural Networks (CNN)\n",
    "\n",
    "7.1 Overall Structure\n",
    "\n",
    "7.2 Convolutional Layers\n",
    "\n",
    "7.3 Pooling Layers\n",
    "\n",
    "7.4 Implementing Convolution/Pooling Layers\n",
    "\n",
    "7.5 Implementing a CNN\n",
    "\n",
    "7.6 Visualizing a CNN\n",
    "\n",
    "7.7 Representative CNNs\n",
    "\n",
    "7.8 Summary\n",
    "\n",
    "CHAPTER 8: Deep Learning\n",
    "\n",
    "8.1 Going Deeper\n",
    "\n",
    "8.2 Early History of Deep Learning\n",
    "\n",
    "8.3 Deep Learning Acceleration (Faster Training)\n",
    "\n",
    "8.4 Applications of Deep Learning\n",
    "\n",
    "8.5 Summary\n",
    "\n",
    "APPENDIX A: Computational Graph of Softmax-with-Loss Layer\n",
    "\n",
    "A.1 Forward Propagation\n",
    "\n",
    "A.2 Backward Propagation\n",
    "\n",
    "A.3 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c500d7f",
   "metadata": {},
   "source": [
    "## CHAPTER 2: Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bafc41",
   "metadata": {},
   "source": [
    "### Mathematical Formulation of a Perceptron\n",
    "A perceptron takes multiple input signals and produces a single output signal. To implement this, we use weights ($w$) and a bias ($b$).    \n",
    "\n",
    "**1. Standard Formula**  \n",
    "The output $y$ is determined by whether the weighted sum of inputs exceeds a certain threshold $\\theta$:$$y = \\begin{cases} 0 & (w_1x_1 + w_2x_2 \\le \\theta) \\\\ 1 & (w_1x_1 + w_2x_2 > \\theta) \\end{cases}$$\n",
    "**2. Formula with Bias**  \n",
    "In practice, we replace the threshold $\\theta$ with a bias ($b$), where $b = -\\theta$. The perceptron fires (outputs 1) if the total sum is greater than 0:$$y = \\begin{cases} 0 & (b + w_1x_1 + w_2x_2 \\le 0) \\\\ 1 & (b + w_1x_1 + w_2x_2 > 0) \\end{cases}$$\n",
    "\n",
    "**Key Components:**\n",
    "* $x_1, x_2$: Input signals\n",
    "* $w_1, w_2$: Weights (Represent the importance of each input)\n",
    "* $b$: Bias (Represents how easily the neuron activates)\n",
    "* $y$: Output signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1, x2):\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    if tmp <= theta:\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c628767",
   "metadata": {},
   "outputs": [],
   "source": [
    "AND(0,0)\n",
    "AND(1,0)\n",
    "AND(0,1)\n",
    "AND(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6084f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
